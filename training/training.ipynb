{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV file paths\n",
    "file_paths = [\n",
    "    # 'Collected Dataset/fist.csv',\n",
    "    # 'Collected Dataset/hand_open.csv',\n",
    "    # 'Collected Dataset/index_middle_thumb.csv',\n",
    "    'Collected Dataset/index_middle.csv',\n",
    "    'Collected Dataset/index_middle2.csv',\n",
    "    # 'Collected Dataset/index_thumb.csv',\n",
    "    'Collected Dataset/index.csv',\n",
    "    # 'Collected Dataset/pinch.csv',\n",
    "    # 'Collected Dataset/peace.csv',\n",
    "    'Collected Dataset/thumbs_pinky.csv',\n",
    "    'Collected Dataset/thumbs_pinky2.csv',\n",
    "    # # 'Collected Dataset/thumbs_down.csv',\n",
    "    # # 'Collected Dataset/thumbs_up.csv',\n",
    "    'Collected Dataset/right/fist.csv',\n",
    "    'Collected Dataset/right/hand_open.csv',\n",
    "    'Collected Dataset/right/index_middle_thumb.csv',\n",
    "    'Collected Dataset/right/index_middle.csv',\n",
    "    'Collected Dataset/right/index_thumb.csv',\n",
    "    'Collected Dataset/right/index.csv',\n",
    "    'Collected Dataset/right/pinch.csv',\n",
    "    'Collected Dataset/right/peace.csv',\n",
    "    'Collected Dataset/right/thumbs_pinky.csv',\n",
    "    'Collected Dataset/right/thumbs_down.csv',\n",
    "    'Collected Dataset/right/thumbs_up.csv',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = pd.concat(csv_data, ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109597, 64)\n"
     ]
    }
   ],
   "source": [
    "# remove missing values\n",
    "df = data.dropna()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramez\\AppData\\Local\\Temp\\ipykernel_25732\\4026723423.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y.replace(string_to_numeric, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(109597, 63)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_numeric = {'index': 0, 'index_middle': 1, 'index_thumb': 2, 'index_middle_thumb' : 3, 'peace': 4, 'hand_open' : 5, 'fist': 6, 'pinch' : 7, 'thumbs_up' : 8, 'thumbs_down': 9 , 'thumbs_pinky' : 10}\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "y.replace(string_to_numeric, inplace=True)\n",
    "\n",
    "X.sample(1)\n",
    "\n",
    "# remove column names\n",
    "X = X.values\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (76717, 63)\n",
      "Testing data shape: (16440, 63)\n",
      "Validation data shape: (16440, 63)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=12)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after PCA: (76717, 21)\n"
     ]
    }
   ],
   "source": [
    "# apply pca\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "\n",
    "print(\"Training data shape after PCA:\", X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pca model\n",
    "import pickle\n",
    "\n",
    "# with open('../utils/pca.pkl', 'wb') as f:\n",
    "#     pickle.dump(pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Hyperparameter tuned models\n",
    "\n",
    "my_models = {\n",
    "    (\"KNN\", KNeighborsClassifier(algorithm = 'auto', n_neighbors = 3)),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(criterion = 'entropy', max_depth = 30, min_samples_split = 2, splitter = 'best')),\n",
    "    (\"Random Forest\", RandomForestClassifier(criterion = 'entropy', max_depth = 20, n_estimators = 50)),\n",
    "    (\"XGBoost\", XGBClassifier(learning_rate=0.2, max_depth=6, n_estimators= 50))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 0.9998175182481752\n",
      "Testing Accuracy: 0.9994525547445255\n",
      "\n",
      "Training Random Forest\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 0.9998175182481752\n",
      "Testing Accuracy: 0.9996350364963503\n",
      "\n",
      "Training KNN\n",
      "Training Accuracy: 0.9999608952383435\n",
      "Validation Accuracy: 0.9997566909975669\n",
      "Testing Accuracy: 0.9996958637469586\n",
      "\n",
      "Training Decision Tree\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 0.9980535279805353\n",
      "Testing Accuracy: 0.9985401459854014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import time \n",
    "\n",
    "for name, model in my_models:\n",
    "    print(\"Training\", name)\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    print(\"Training Accuracy:\", model.score(X_train_pca, y_train))\n",
    "    print(\"Validation Accuracy:\", model.score(X_val_pca, y_val))\n",
    "    print(\"Testing Accuracy:\", model.score(X_test_pca, y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBoost\n",
      "Time taken for prediction: 0.05799722671508789\n",
      "[[1953    2    0    0    0    0    0    0    0    0    0]\n",
      " [   1 2621    0    0    0    0    3    0    0    0    0]\n",
      " [   0    0 1388    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 1426    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0 1443    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0 1244    0    0    0    0    0]\n",
      " [   2    0    0    0    0    0  995    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 1233    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  862    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 1001    0]\n",
      " [   0    0    0    0    0    1    0    0    0    0 2265]]\n",
      "\n",
      "Testing Accuracy: 0.9994525547445255\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Time taken for prediction: 0.10700225830078125\n",
      "[[1955    0    0    0    0    0    0    0    0    0    0]\n",
      " [   3 2619    0    0    0    0    3    0    0    0    0]\n",
      " [   0    0 1388    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 1426    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0 1443    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0 1244    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0  997    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 1233    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  862    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 1001    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0 2266]]\n",
      "\n",
      "Testing Accuracy: 0.9996350364963503\n",
      "\n",
      "\n",
      "Model: KNN\n",
      "Time taken for prediction: 1.911003828048706\n",
      "[[1954    0    0    0    1    0    0    0    0    0    0]\n",
      " [   1 2621    0    0    0    0    3    0    0    0    0]\n",
      " [   0    0 1388    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 1426    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0 1443    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0 1244    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0  997    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 1233    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  862    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 1001    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0 2266]]\n",
      "\n",
      "Testing Accuracy: 0.9996958637469586\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Time taken for prediction: 0.008001327514648438\n",
      "[[1949    1    0    0    1    2    2    0    0    0    0]\n",
      " [   2 2622    0    0    0    0    1    0    0    0    0]\n",
      " [   0    0 1385    2    0    0    0    0    0    1    0]\n",
      " [   0    0    1 1422    0    2    1    0    0    0    0]\n",
      " [   1    0    0    0 1442    0    0    0    0    0    0]\n",
      " [   1    0    1    0    0 1242    0    0    0    0    0]\n",
      " [   1    0    0    1    0    0  995    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 1233    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  862    0    0]\n",
      " [   0    0    0    0    0    0    1    1    0  999    0]\n",
      " [   0    0    0    1    0    0    0    0    0    0 2265]]\n",
      "\n",
      "Testing Accuracy: 0.9985401459854014\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in my_models:\n",
    "    print(\"Model:\", name)\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    end = time.time()\n",
    "    print(\"Time taken for prediction:\", end-start)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print()\n",
    "    print(\"Testing Accuracy:\", model.score(X_test_pca, y_test))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  8  5  6 10  0  9  2  3  7  4]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "num_states = len(y.unique())\n",
    "\n",
    "print(y.unique())\n",
    "\n",
    "tf_model = models.Sequential([\n",
    "    layers.Input(shape=(X_train_pca.shape[1],)), \n",
    "    # tf.keras.layers.Dense(24, activation= 'relu'), \n",
    "    tf.keras.layers.Dense(64, activation= 'relu'), \n",
    "    # tf.keras.layers.Dense(128, activation= 'relu'), \n",
    "    tf.keras.layers.Dense(num_states, activation= 'softmax')\n",
    "])\n",
    "                                \n",
    "# Compile the model\n",
    "tf_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8124 - loss: 0.7733 - val_accuracy: 0.9901 - val_loss: 0.0515\n",
      "Epoch 2/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9910 - loss: 0.0408 - val_accuracy: 0.9922 - val_loss: 0.0304\n",
      "Epoch 3/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0256 - val_accuracy: 0.9940 - val_loss: 0.0217\n",
      "Epoch 4/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9948 - loss: 0.0187 - val_accuracy: 0.9953 - val_loss: 0.0169\n",
      "Epoch 5/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0149 - val_accuracy: 0.9957 - val_loss: 0.0138\n",
      "Epoch 6/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9963 - loss: 0.0127 - val_accuracy: 0.9970 - val_loss: 0.0118\n",
      "Epoch 7/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0092 - val_accuracy: 0.9970 - val_loss: 0.0102\n",
      "Epoch 8/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0087 - val_accuracy: 0.9977 - val_loss: 0.0087\n",
      "Epoch 9/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9980 - loss: 0.0077 - val_accuracy: 0.9976 - val_loss: 0.0077\n",
      "Epoch 10/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0067 - val_accuracy: 0.9977 - val_loss: 0.0074\n",
      "Epoch 11/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0059 - val_accuracy: 0.9981 - val_loss: 0.0070\n",
      "Epoch 12/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 0.9984 - val_loss: 0.0054\n",
      "Epoch 13/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 0.9982 - val_loss: 0.0062\n",
      "Epoch 14/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 0.9987 - val_loss: 0.0046\n",
      "Epoch 15/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 0.9987 - val_loss: 0.0044\n",
      "Epoch 16/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 0.9987 - val_loss: 0.0039\n",
      "Epoch 17/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 0.9990 - val_loss: 0.0038\n",
      "Epoch 18/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 0.9987 - val_loss: 0.0036\n",
      "Epoch 19/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 0.9991 - val_loss: 0.0029\n",
      "Epoch 20/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 0.9989 - val_loss: 0.0030\n",
      "Epoch 21/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 0.9991 - val_loss: 0.0025\n",
      "Epoch 22/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 0.9992 - val_loss: 0.0024\n",
      "Epoch 23/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0018 - val_accuracy: 0.9993 - val_loss: 0.0025\n",
      "Epoch 24/50\n",
      "\u001b[1m2398/2398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 0.9991 - val_loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = tf_model.fit(\n",
    "    # X_train, y_train,\n",
    "    X_train_pca, y_train,\n",
    "    epochs=50,\n",
    "    # validation_data=(X_val, y_val),\n",
    "    validation_data=(X_val_pca, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prediction: 0.017004013061523438\n",
      "[[1952    2    0    0    0    0    1    0    0    0    0]\n",
      " [   3 2622    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1388    0    0    0    0    0    0    0    0]\n",
      " [   0    0    1 1425    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0 1443    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0 1244    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0  997    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 1233    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  862    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 1001    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0 2266]]\n",
      "\n",
      "Accuracy: 0.9995742092457421\n"
     ]
    }
   ],
   "source": [
    "# draw confusion matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "y_pred = tf_model(X_test_pca)\n",
    "end = time.time()\n",
    "print(\"Time taken for prediction:\", end-start)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nAccuracy:\", np.trace(cm) / np.sum(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, model in my_models:\n",
    "#     # if name == \"Random Forest\":\n",
    "#     if name == \"Decision Tree\":\n",
    "#         import pickle\n",
    "#         with open('../models/d_tree.pkl', 'wb') as f:\n",
    "#             pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "tf_model.save('../models/tfv4.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Virtual Mouse - Hand Tracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
